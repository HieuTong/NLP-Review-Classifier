{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Tong Minh Hieu Le\n",
    "#### Student ID: 4098368\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* RegexpTokenizer \n",
    "* chain\n",
    "* ...\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this file, we perform basic text pre-processing on the given dataset, including, but not limited to tokenization, removing most/least frequent words and stop words. In this task, we focus on pre-processing the “Review Text” only.\n",
    "1. Extract information about the review. Perform the pre-processing steps mentioned below to the extracted reviews\n",
    "2. Tokenize each clothing review. The word tokenization must use the following regular expression, r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\";\n",
    "3. All the words must be converted into the lower case;\n",
    "4. Remove words with a length less than 2.\n",
    "5. Remove stopwords using the provided stop words list (i.e., stopwords_en.txt). It is located\n",
    "inside the same downloaded folder.\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "7. Remove the top 20 most frequent words based on document frequency.\n",
    "8. Save the processed data as processed.csv file.\n",
    "9. Build a vocabulary of the cleaned/processed reviews, and save it in a txt file (please refer to the\n",
    "Required Output section);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data and explain your findings\n",
    "- Load the data into proper data structures and get it ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to inspect the provided data file...\n",
    "# Loading the data\n",
    "df = pd.read_csv('assignment3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19662, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General         Dresses    Dresses  \n",
       "1                        0  General Petite         Bottoms      Pants  \n",
       "2                        6         General            Tops    Blouses  \n",
       "3                        4         General         Dresses    Dresses  \n",
       "4                        1  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to check for missing values \n",
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clothing ID                 int64\n",
       "Age                         int64\n",
       "Title                      object\n",
       "Review Text                object\n",
       "Rating                      int64\n",
       "Recommended IND             int64\n",
       "Positive Feedback Count     int64\n",
       "Division Name              object\n",
       "Department Name            object\n",
       "Class Name                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19662.000000</td>\n",
       "      <td>19662.000000</td>\n",
       "      <td>19662.000000</td>\n",
       "      <td>19662.000000</td>\n",
       "      <td>19662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>921.297274</td>\n",
       "      <td>43.260808</td>\n",
       "      <td>4.183145</td>\n",
       "      <td>0.818177</td>\n",
       "      <td>2.652477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>200.227528</td>\n",
       "      <td>12.258122</td>\n",
       "      <td>1.112224</td>\n",
       "      <td>0.385708</td>\n",
       "      <td>5.834285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>861.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>936.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1078.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1205.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Clothing ID           Age        Rating  Recommended IND  \\\n",
       "count  19662.000000  19662.000000  19662.000000     19662.000000   \n",
       "mean     921.297274     43.260808      4.183145         0.818177   \n",
       "std      200.227528     12.258122      1.112224         0.385708   \n",
       "min        1.000000     18.000000      1.000000         0.000000   \n",
       "25%      861.000000     34.000000      4.000000         1.000000   \n",
       "50%      936.000000     41.000000      5.000000         1.000000   \n",
       "75%     1078.000000     52.000000      5.000000         1.000000   \n",
       "max     1205.000000     99.000000      5.000000         1.000000   \n",
       "\n",
       "       Positive Feedback Count  \n",
       "count             19662.000000  \n",
       "mean                  2.652477  \n",
       "std                   5.834285  \n",
       "min                   0.000000  \n",
       "25%                   0.000000  \n",
       "50%                   1.000000  \n",
       "75%                   3.000000  \n",
       "max                 122.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps.\n",
    "\n",
    "1. **Extract review information**: Extract the text from the \"Review Text\" column for processing.\n",
    "\n",
    "2. **Tokenize reviews**: Use the regular expression `r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"` to tokenize each clothing review into individual words.\n",
    "\n",
    "3. **Convert to lowercase**: Transform all words to lowercase to ensure consistency.\n",
    "\n",
    "4. **Remove short words**: Remove words with length less than 2 characters.\n",
    "\n",
    "5. **Remove stopwords**: Filter out common stopwords using the provided stopwords_en.txt file.\n",
    "\n",
    "6. **Remove infrequent words**: Eliminate words that appear only once in the entire collection.\n",
    "\n",
    "7. **Remove most frequent words**: Remove the top 20 most frequent words based on document frequency.\n",
    "\n",
    "8. **Save processed data**: Store the processed reviews in a CSV file named \"processed.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Extract review information**: Extract the text from the \"Review Text\" column for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 + 1.2.3: Tokenize reviews and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeReview(raw_review):\n",
    "    \"\"\"\n",
    "    This function converts all words to lowercase,\n",
    "    segments the raw review into sentences, tokenizes each sentence\n",
    "    and returns the review as a list of tokens.\n",
    "    \"\"\"\n",
    "    # Handle NaN or non-string values\n",
    "    if not isinstance(raw_review, str):\n",
    "        return []\n",
    "        \n",
    "    nl_review = raw_review.lower()  # convert all words to lowercase\n",
    "    \n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(nl_review)\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "    \n",
    "    # merge them into a list of tokens\n",
    "    tokenised_review = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_reviews = [tokenizeReview(r) for r in reviews]  # list comprehension, generate a list of tokenized articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_print(tk_reviews):\n",
    "    words = list(chain.from_iterable(tk_reviews)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(tk_reviews))\n",
    "    lens = [len(article) for article in tk_reviews]\n",
    "    print(\"Average review length:\", np.mean(lens))\n",
    "    print(\"Maximun review length:\", np.max(lens))\n",
    "    print(\"Minimun review length:\", np.min(lens))\n",
    "    print(\"Standard deviation of review length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to test element in tk reviews\n",
    "test_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review:\n",
      " I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments! \n",
      "\n",
      "Tokenized review:\n",
      " ['i', 'love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'i', 'wear', 'it', 'i', 'get', 'nothing', 'but', 'great', 'compliments']\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw review:\\n\",reviews[test_index],'\\n')\n",
    "print(\"Tokenized review:\\n\",tk_reviews[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  14806\n",
      "Total number of tokens:  1206688\n",
      "Lexical diversity:  0.012269948818584423\n",
      "Total number of reviews: 19662\n",
      "Average review length: 61.37157969687723\n",
      "Maximun review length: 113\n",
      "Minimun review length: 2\n",
      "Standard deviation of review length: 27.802596969841698\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. Remove short words**: Remove words with length less than 2 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out single character tokens\n",
    "tk_reviews = [[w for w in review if len(w) >=2] \\\n",
    "                      for review in tk_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized review:\n",
      " ['love', 'love', 'love', 'this', 'jumpsuit', \"it's\", 'fun', 'flirty', 'and', 'fabulous', 'every', 'time', 'wear', 'it', 'get', 'nothing', 'but', 'great', 'compliments']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized review:\\n\",tk_reviews[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  14780\n",
      "Total number of tokens:  1109634\n",
      "Lexical diversity:  0.013319707218776641\n",
      "Total number of reviews: 19662\n",
      "Average review length: 56.43545926151968\n",
      "Maximun review length: 104\n",
      "Minimun review length: 2\n",
      "Standard deviation of review length: 25.39546596696992\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5. Remove stopwords**: Filter out common stopwords using the provided stopwords_en.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words\n",
    "with open('stopwords_en.txt', 'r') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized review after removing stopwords:\n",
      " ['love', 'love', 'love', 'jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'wear', 'great', 'compliments']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords from tokenized reviews\n",
    "tk_reviews = [[w for w in review if w not in stopwords] \n",
    "              for review in tk_reviews]\n",
    "\n",
    "# Check the result on the sample review\n",
    "print(\"Tokenized review after removing stopwords:\\n\", tk_reviews[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  14283\n",
      "Total number of tokens:  452692\n",
      "Lexical diversity:  0.031551253390826435\n",
      "Total number of reviews: 19662\n",
      "Average review length: 23.023700539110976\n",
      "Maximun review length: 51\n",
      "Minimun review length: 1\n",
      "Standard deviation of review length: 10.165913222944233\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6. Remove infrequent words**: Eliminate words that appear only once in the document collection, based on term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tk_reviews)) # we put all the tokens in the corpus in a single list\n",
    "term_freq = FreqDist(words) # compute the term frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hulked',\n",
       " 'geiger',\n",
       " 'incidents',\n",
       " 'over-indulge',\n",
       " 'over-stretch',\n",
       " 'goods',\n",
       " 'incr',\n",
       " 'whips',\n",
       " 'whispered',\n",
       " 'creativity',\n",
       " 'goto',\n",
       " 'sweathsirt',\n",
       " 'top-it',\n",
       " 'unforgettable',\n",
       " 'worldly',\n",
       " 'withhold',\n",
       " 'makes-you',\n",
       " 'slouch-y',\n",
       " 'regardi',\n",
       " 'taupe-totally',\n",
       " 'nandita',\n",
       " 'relentless',\n",
       " 'supporting',\n",
       " 'cold-its',\n",
       " 'clinton',\n",
       " 'mensware',\n",
       " \"petite's\",\n",
       " 'attachments',\n",
       " 's-on',\n",
       " 'gateway',\n",
       " 'gypsy',\n",
       " 'tonal',\n",
       " 'non-collar',\n",
       " 'embarrassed',\n",
       " 'discolored',\n",
       " 'announce',\n",
       " 'vribant',\n",
       " 'sillhoette',\n",
       " 'lampshades',\n",
       " 'fluttered',\n",
       " 'pashminas',\n",
       " \"stylist's\",\n",
       " \"sevigny's\",\n",
       " 'sale-only',\n",
       " 'ding',\n",
       " 'propensity',\n",
       " 'juuuuussst',\n",
       " 'lef',\n",
       " 'core',\n",
       " 'possess',\n",
       " 'ubiquitous',\n",
       " 'vinta',\n",
       " 'hermosa',\n",
       " 'shel',\n",
       " 'guidance',\n",
       " 'pantyhose',\n",
       " 'unforced',\n",
       " 'shield',\n",
       " 'fisherman',\n",
       " 'escalante',\n",
       " 'thinne',\n",
       " 'lower-cut',\n",
       " 'invent',\n",
       " 'groin',\n",
       " 'rico',\n",
       " 'notwithstanding',\n",
       " 'camouflaged',\n",
       " 'succulents',\n",
       " 'deceided',\n",
       " 'rebought',\n",
       " 'remarks',\n",
       " 'flowered-design',\n",
       " 'cooking',\n",
       " 'jaw',\n",
       " 'enjoyment',\n",
       " 'pop-it',\n",
       " 'jeans-super',\n",
       " 'incomparable',\n",
       " 'beardedlady',\n",
       " 'reviews-but',\n",
       " 'ballerina',\n",
       " 'pliers',\n",
       " 'kindest',\n",
       " 'plug',\n",
       " 'pebbled',\n",
       " 'collars',\n",
       " 'downplaying',\n",
       " 'connector',\n",
       " 'pta',\n",
       " 'lop-sided',\n",
       " 'dazzling',\n",
       " 'purpleish',\n",
       " 'fluxes',\n",
       " 'lacy-show',\n",
       " 'fitting-too',\n",
       " 'ho',\n",
       " 'alto',\n",
       " 'space-age',\n",
       " \"lab's\",\n",
       " 'vegan-friendly',\n",
       " 'tea-length',\n",
       " 'registry',\n",
       " 'reticketed',\n",
       " 're-purchase',\n",
       " 'amost',\n",
       " 'reflective',\n",
       " 'swollen',\n",
       " 'pinstripes',\n",
       " 'overruled',\n",
       " 'morocco',\n",
       " 'economical',\n",
       " 'collarbones',\n",
       " 'mentione',\n",
       " 'system',\n",
       " 'compliement',\n",
       " 'ivory-colored',\n",
       " 'blew',\n",
       " 'merchants',\n",
       " 'raggy',\n",
       " 'swapping',\n",
       " 'shirtsleeves',\n",
       " 'washa',\n",
       " \"la's\",\n",
       " 'ivory-isn',\n",
       " \"summer's\",\n",
       " 'timy',\n",
       " 'comforta',\n",
       " 'merging',\n",
       " 'one-two',\n",
       " 'neckli',\n",
       " 'mediately',\n",
       " 'geez',\n",
       " 'brave',\n",
       " 'foolishly',\n",
       " 'prize',\n",
       " 'homework',\n",
       " 'hourgass',\n",
       " 'cane',\n",
       " 'relief',\n",
       " 'ill-placed',\n",
       " 'counterbalance',\n",
       " 'boobies',\n",
       " 'walters',\n",
       " 'thrille',\n",
       " 'crushing',\n",
       " 'fancied',\n",
       " 'topping',\n",
       " 'valid',\n",
       " 'acquiring',\n",
       " 'allergies',\n",
       " 'winding',\n",
       " 'wintergreens',\n",
       " 'sexiest',\n",
       " 'collar-bone',\n",
       " 'websites',\n",
       " 'soft-light',\n",
       " 'greeny',\n",
       " 'ro',\n",
       " 'blisters',\n",
       " 'oclor',\n",
       " 'yellow-ivory',\n",
       " 'retailerpologists',\n",
       " 'patient',\n",
       " 'weaver',\n",
       " 'haunt',\n",
       " 'ahtro',\n",
       " 'ber',\n",
       " 'downed',\n",
       " 'lux',\n",
       " 'rial',\n",
       " 'transistional',\n",
       " 'oohs',\n",
       " 'chokes',\n",
       " 'different-length',\n",
       " 'figure-defining',\n",
       " 'conservatively',\n",
       " 'pigmented',\n",
       " 'vast',\n",
       " 'make-ups',\n",
       " 'military-ish',\n",
       " 'too-there',\n",
       " 'suppre',\n",
       " 'yellow-beige',\n",
       " 'tranforms',\n",
       " 'sallow',\n",
       " 'reviewe',\n",
       " 'span',\n",
       " 'clown-like',\n",
       " 'veered',\n",
       " 'sunblock',\n",
       " 'oppressive',\n",
       " 'causally',\n",
       " 'spandex-y',\n",
       " 'wayyyyy',\n",
       " 'embroider',\n",
       " 'bra-friendliness',\n",
       " 'professionalism',\n",
       " 'pre-holiday',\n",
       " 'accepts',\n",
       " 'sized-it',\n",
       " 'lime-sherbet',\n",
       " 'gender',\n",
       " 'neckhole',\n",
       " 'ss',\n",
       " 'nottom',\n",
       " 'flouncier',\n",
       " 'storeroom',\n",
       " 'slim-fitting',\n",
       " 'old-style',\n",
       " 'otherwsie',\n",
       " 'creeping',\n",
       " 'face-framing',\n",
       " 'classroom',\n",
       " 'valentine',\n",
       " 'marred',\n",
       " 'surfing',\n",
       " 'pilcr',\n",
       " 'theblack',\n",
       " 'impeded',\n",
       " 'retire',\n",
       " 'eccentric',\n",
       " 'reviewwers',\n",
       " 'brilliant-the',\n",
       " 'dividing',\n",
       " 'monkey',\n",
       " 'them-but',\n",
       " 'sinc',\n",
       " 'anyw',\n",
       " 'merlot',\n",
       " 'pulles',\n",
       " 'minuscule',\n",
       " 'trashed',\n",
       " 'suprise',\n",
       " 'unreal',\n",
       " 'argument',\n",
       " 'beautful',\n",
       " 'exams',\n",
       " 'striked',\n",
       " 'wrin',\n",
       " 'birks',\n",
       " 'odette',\n",
       " 'webbed',\n",
       " 'mayb',\n",
       " 'someth',\n",
       " 'tank-it',\n",
       " 'waking',\n",
       " 'embroid',\n",
       " 'wich',\n",
       " 'baggage',\n",
       " 'flare-it',\n",
       " 'ripple-front',\n",
       " 'ws',\n",
       " 'lounges',\n",
       " 'dinosaur',\n",
       " 'gumby',\n",
       " 'hand-washable',\n",
       " 'grrrrr',\n",
       " \"aaaaandidon'twanttopayforshipping\",\n",
       " 'probable',\n",
       " 'multi-seasonal',\n",
       " 'explaining',\n",
       " 'suffocating',\n",
       " 'overlapped',\n",
       " 'uno',\n",
       " 'pricing',\n",
       " 'commissioned',\n",
       " 'waiat',\n",
       " 'rechecking',\n",
       " 'eucalan',\n",
       " 'adoring',\n",
       " 'asheville',\n",
       " 'earl',\n",
       " 'loosness',\n",
       " 'whoops',\n",
       " 'heavy-lifting',\n",
       " 'weeding',\n",
       " 'tame',\n",
       " 'coplaints',\n",
       " 'evokes',\n",
       " 'field',\n",
       " 'wavy-distorted',\n",
       " 'lightheaded',\n",
       " 'sitter',\n",
       " 'succumb',\n",
       " 'blessing',\n",
       " 'yey',\n",
       " 'carol',\n",
       " 'flowy-ness',\n",
       " 'poofier',\n",
       " 'lait',\n",
       " \"arm's\",\n",
       " 'inconvenient',\n",
       " 'nothings',\n",
       " 'allowance',\n",
       " 'defectiv',\n",
       " 'frocks',\n",
       " 'aid',\n",
       " 'psuedo-empire',\n",
       " 'the-mill',\n",
       " 'operatic',\n",
       " 'oder',\n",
       " 'tattered',\n",
       " 'buns',\n",
       " 'smash',\n",
       " 'michigan',\n",
       " 'cozy-around',\n",
       " 'damask',\n",
       " 'bluish-green',\n",
       " 'arched',\n",
       " 'sooooooooooooooo',\n",
       " 'bombshell',\n",
       " 'humor',\n",
       " \"i'ts\",\n",
       " 'swing-ness',\n",
       " 'resembled',\n",
       " 'crystals',\n",
       " 'strech',\n",
       " 'yellow-gold',\n",
       " 'havent',\n",
       " 'symphony',\n",
       " 'mayer',\n",
       " 'conceptually',\n",
       " 'declared',\n",
       " 'all-round',\n",
       " 'pattten',\n",
       " 'angela',\n",
       " 'comb',\n",
       " 'notation',\n",
       " 'long-again',\n",
       " 'undnerneath',\n",
       " 'sorcha',\n",
       " 'corpse',\n",
       " 'wreaked',\n",
       " 'halterneck',\n",
       " 'likded',\n",
       " 'greedy',\n",
       " 'aside-since',\n",
       " 'poufing',\n",
       " 'flyi',\n",
       " 'selvage',\n",
       " 'adjsut',\n",
       " 'fastener',\n",
       " 'cross-fit',\n",
       " 'otherw',\n",
       " 'jewel-like',\n",
       " 'lysee',\n",
       " 'symptoms',\n",
       " 'sheek',\n",
       " 'skating',\n",
       " 'problem-i',\n",
       " 'grave',\n",
       " 'encouraging',\n",
       " 'unsuccessful',\n",
       " 's-ski',\n",
       " 'quicker',\n",
       " 'seychelles',\n",
       " 'de-emphasizes',\n",
       " 'long-so',\n",
       " 'snigger',\n",
       " 'proving',\n",
       " 'aaahed',\n",
       " 'analysis',\n",
       " 'tattooed',\n",
       " 'odd-length',\n",
       " 'kimono-style',\n",
       " 'frabic',\n",
       " 'lcan',\n",
       " 'poodles',\n",
       " 'scree',\n",
       " 'powering',\n",
       " 'autobots',\n",
       " 'weddingrehearsal',\n",
       " 'mysteries',\n",
       " 'duster-length',\n",
       " 'sliming',\n",
       " 'retro-swingy',\n",
       " 'watercolor-like',\n",
       " 'reward',\n",
       " 'shh',\n",
       " 'boodie',\n",
       " 'cecelia',\n",
       " 'bra-type',\n",
       " 'aaaaannnnnnd',\n",
       " 'hurriedly',\n",
       " 'buys',\n",
       " 're-visit',\n",
       " \"mumu's\",\n",
       " 'stickers',\n",
       " 'tank-in',\n",
       " 'high-tops',\n",
       " 'cahrm',\n",
       " 'firming',\n",
       " 'chiquita',\n",
       " 'racier',\n",
       " 'spacedye',\n",
       " 'satisfactory',\n",
       " 'saldly',\n",
       " \"medium's\",\n",
       " 'wars',\n",
       " 'andddddd',\n",
       " 'toggles',\n",
       " 'havin',\n",
       " 'caftans',\n",
       " 'spot-cleaned',\n",
       " 'kittens',\n",
       " 'cute-great',\n",
       " 'ironic',\n",
       " 'addtl',\n",
       " 'overstitching',\n",
       " 'peace',\n",
       " 'foo-foo',\n",
       " 'peittes',\n",
       " 'omits',\n",
       " 'pristine',\n",
       " 'taggart',\n",
       " 'retreat',\n",
       " 'giantess',\n",
       " 'air-conditioned',\n",
       " 'promos',\n",
       " 'flax',\n",
       " 'hands-down',\n",
       " 'shelves',\n",
       " 'ljsut',\n",
       " 'rush',\n",
       " 'transcends',\n",
       " 'c-sections',\n",
       " 'area-win',\n",
       " 'branded',\n",
       " 'well-structured',\n",
       " 'dropwaist',\n",
       " 'full-on',\n",
       " 'speaker',\n",
       " 'darted',\n",
       " 'climb',\n",
       " 'pillowy',\n",
       " 'treatments',\n",
       " 'bohemain',\n",
       " 'ed',\n",
       " 'acceptabl',\n",
       " 'reeked',\n",
       " 'pos',\n",
       " 'blue-which',\n",
       " 'move-ability',\n",
       " 'confirmed',\n",
       " 'therapy',\n",
       " 'waaaaaay',\n",
       " 'non-fussy',\n",
       " 'flutes',\n",
       " 'font',\n",
       " 'demential',\n",
       " 'raindrops',\n",
       " 'pirchasing',\n",
       " 'fringe-like',\n",
       " 'gogo',\n",
       " 'toronto-british',\n",
       " 'waaaaaaaay',\n",
       " 'small-shouldered',\n",
       " 'short-shorts',\n",
       " 'astronomical',\n",
       " 'spacek',\n",
       " 'daisy',\n",
       " 'mainstay',\n",
       " 'independently',\n",
       " 'top-that',\n",
       " 'predictable',\n",
       " 'tripping',\n",
       " 'lust',\n",
       " 'ventured',\n",
       " 'shuffle',\n",
       " 'odering',\n",
       " 'barnyard',\n",
       " 'wiskering',\n",
       " 'farrier',\n",
       " 'photography',\n",
       " 'jams',\n",
       " 'cowgirl',\n",
       " 'underdress',\n",
       " 'enclosed',\n",
       " 'mat',\n",
       " 'alert',\n",
       " 'swing-y',\n",
       " \"sa's\",\n",
       " 'matted',\n",
       " 'out-lined',\n",
       " 'seas',\n",
       " 'iris',\n",
       " 'knuc',\n",
       " 'cluster',\n",
       " 'straightforward',\n",
       " 'environs',\n",
       " 'madeline',\n",
       " 'alr',\n",
       " 'unattached',\n",
       " 'enlarging',\n",
       " 'perches',\n",
       " 'vineyard',\n",
       " 'sprea',\n",
       " 'strokes',\n",
       " 'tolerating',\n",
       " 'sofla',\n",
       " 'accord',\n",
       " 'ageless',\n",
       " 'wonderfully-made',\n",
       " 'spade',\n",
       " 'conspicuously',\n",
       " 'metallics',\n",
       " 'crimped',\n",
       " 'proably',\n",
       " 'explosing',\n",
       " \"back's\",\n",
       " 'terra-cotta',\n",
       " 'flea',\n",
       " 'pvc',\n",
       " 'relent',\n",
       " 'categories',\n",
       " 'work-outs',\n",
       " 'buttah',\n",
       " 'warns',\n",
       " 'clincher',\n",
       " 'rsty',\n",
       " 'peaches',\n",
       " 'sierra',\n",
       " 'pouffy',\n",
       " \"jammie's\",\n",
       " 'upping',\n",
       " 'crave',\n",
       " 'ama',\n",
       " 'shirtwaist',\n",
       " 'geek',\n",
       " 'oprefer',\n",
       " 'exclaimed',\n",
       " 'loro',\n",
       " 'incidental',\n",
       " 'noir',\n",
       " 'netural',\n",
       " 'jelly',\n",
       " 'sinched',\n",
       " 'stat',\n",
       " 'med-large',\n",
       " 'hula',\n",
       " 'to-christmas',\n",
       " 'lumpy-looking',\n",
       " 'foot-long',\n",
       " \"hemline's\",\n",
       " 'hammy',\n",
       " 'enjoyable',\n",
       " 'irradiated',\n",
       " 'lovel',\n",
       " 'unfortunetly',\n",
       " 'mis-sized',\n",
       " 'pinstriped',\n",
       " 'gray-tone',\n",
       " \"preschooler's\",\n",
       " 'face-just',\n",
       " 'medium-light',\n",
       " 'advises',\n",
       " 'long-torsoed',\n",
       " 'goon',\n",
       " 'mis',\n",
       " 'bejewels',\n",
       " 'misrepresent',\n",
       " 'quadruplets',\n",
       " 'deciced',\n",
       " 'polarized',\n",
       " 'balloon-fit',\n",
       " 'cream-ish',\n",
       " 'purple-ish',\n",
       " 'sweeater',\n",
       " 'frmae',\n",
       " 'overhead',\n",
       " 'spades',\n",
       " 'dolor',\n",
       " 'recomnend',\n",
       " 'nigh',\n",
       " 'canna',\n",
       " 'plusher',\n",
       " 'night-worthy',\n",
       " 'finially',\n",
       " 'asymetrical',\n",
       " 'volition',\n",
       " 'high-water',\n",
       " 'prettily',\n",
       " 'les',\n",
       " 'seasonality',\n",
       " 'wt',\n",
       " 'btoh',\n",
       " 'ple',\n",
       " 'sizw',\n",
       " 'latch',\n",
       " 'piece-the',\n",
       " 'photo-open',\n",
       " 'deminished',\n",
       " 'differemt',\n",
       " 'horror',\n",
       " 'colletage',\n",
       " 'conflict',\n",
       " 'appropriateness',\n",
       " 'lstretch',\n",
       " 'unifying',\n",
       " 'xxxs',\n",
       " 'narrowis',\n",
       " 'substituting',\n",
       " 'alce',\n",
       " 'flights',\n",
       " 'scouting',\n",
       " 'peep-toe',\n",
       " 'cable-knot',\n",
       " 'phoenix',\n",
       " 'insult',\n",
       " 'deresses',\n",
       " 'stratosphere',\n",
       " 'nabbing',\n",
       " 'incomplete',\n",
       " 'sillouhette',\n",
       " 'emobroidery',\n",
       " 'references',\n",
       " 'artificial',\n",
       " 'rippled',\n",
       " 'boosh',\n",
       " 'naming',\n",
       " 'vintage-gone',\n",
       " 'dismayed',\n",
       " 'unpack',\n",
       " 'piecing',\n",
       " 'normal-sized',\n",
       " 'splashy',\n",
       " 'nat',\n",
       " 'ears',\n",
       " 'pefect',\n",
       " 'native',\n",
       " 'admiration',\n",
       " 'vintage-made',\n",
       " 'lai',\n",
       " 'imlove',\n",
       " 'gauze-y',\n",
       " 'loungin',\n",
       " 'oft',\n",
       " \"dnd't\",\n",
       " 'nutty',\n",
       " 'blute',\n",
       " 'vacating',\n",
       " 'mongolian',\n",
       " 'tic',\n",
       " 'proposed',\n",
       " 'curb',\n",
       " 'oot',\n",
       " 'cars',\n",
       " 'all-purpose',\n",
       " 'persimmon',\n",
       " 'hallway',\n",
       " 'straight-on',\n",
       " 'buildings',\n",
       " 'pampered',\n",
       " 'ni',\n",
       " 'rt',\n",
       " 'short-ish',\n",
       " 'iordered',\n",
       " 'shininess',\n",
       " 'pbly',\n",
       " 'fluorescent',\n",
       " 'petittes',\n",
       " 'gorgeousness',\n",
       " 'dwarfed',\n",
       " 'square-fitting',\n",
       " 'stiffly',\n",
       " 'dummy',\n",
       " 'rooster',\n",
       " 'flamenco',\n",
       " 'ribboned',\n",
       " 'v-opening',\n",
       " 'tequila-ginger',\n",
       " 'anothr',\n",
       " 'wering',\n",
       " 'flory',\n",
       " 'bitsy',\n",
       " 'drooling',\n",
       " 'bip',\n",
       " 'availalbe',\n",
       " 'resolution',\n",
       " 'newbury',\n",
       " 'collections',\n",
       " 'tot',\n",
       " 'sink',\n",
       " 'plus-sized',\n",
       " 'excell',\n",
       " 'baskets',\n",
       " 'vita',\n",
       " 'love-hate',\n",
       " 'sewers',\n",
       " 'quarters',\n",
       " 'lounge-at',\n",
       " 'whyyyyyyyyyyy',\n",
       " 'crass',\n",
       " 'puerto',\n",
       " 'bei',\n",
       " 'oatmealish-warm',\n",
       " 'labelled',\n",
       " 'international',\n",
       " 'faced',\n",
       " 'sb',\n",
       " 'waist-it',\n",
       " 'fuzzes',\n",
       " 'lsit',\n",
       " 'davy',\n",
       " 'heavier-weight',\n",
       " 'coope',\n",
       " 'beg',\n",
       " 'movies',\n",
       " 'copy',\n",
       " 'proud',\n",
       " 'remark',\n",
       " 'skews',\n",
       " 'papaya',\n",
       " 'wotb',\n",
       " 'mommy-tummy',\n",
       " 'balc',\n",
       " 'pouch-y',\n",
       " 'single-use',\n",
       " 'dit',\n",
       " 'daydreaming',\n",
       " 'eden',\n",
       " 'reacts',\n",
       " 'hr',\n",
       " 'instantaneously',\n",
       " 'breastline',\n",
       " 'occurred',\n",
       " 'bahama',\n",
       " 'yellowed',\n",
       " 'unsolvable',\n",
       " 'ecause',\n",
       " \"summer'ness\",\n",
       " 'jt',\n",
       " 'cleani',\n",
       " 'arctic',\n",
       " 'relies',\n",
       " 'thruout',\n",
       " 'on-seam',\n",
       " 'stage',\n",
       " 'disturb',\n",
       " 'tum',\n",
       " 'big-i',\n",
       " 'maternity-look',\n",
       " 'minny',\n",
       " 'kuddos',\n",
       " 'rayon-gauze',\n",
       " 'surroundings',\n",
       " 'cloying',\n",
       " 'pancho',\n",
       " 'nose',\n",
       " 'reveries',\n",
       " 'sungarden',\n",
       " 'reseam',\n",
       " 'supremely',\n",
       " 'jagged',\n",
       " 'royalty',\n",
       " 'sweather',\n",
       " 'presentable',\n",
       " 'untanned',\n",
       " 'unbuttons',\n",
       " 'versatile-so',\n",
       " 'thakoon',\n",
       " 'cheezy',\n",
       " 'worker',\n",
       " 'vibrant-true',\n",
       " 'poooofffy',\n",
       " 'jacksons',\n",
       " 'resemblance',\n",
       " 'intrinsic',\n",
       " 'unprotected',\n",
       " 'continuously',\n",
       " 'nbeed',\n",
       " 'tv',\n",
       " 'embrodering',\n",
       " 'raccoons',\n",
       " 'disapointed',\n",
       " 'respectable',\n",
       " 'berries',\n",
       " 'specifies',\n",
       " 'master',\n",
       " 'cranny',\n",
       " 'delegate',\n",
       " 'normaly',\n",
       " 'faux-leather',\n",
       " 'spandex-there',\n",
       " 'frnt',\n",
       " 'tushy',\n",
       " 'unravelling',\n",
       " 'in-line',\n",
       " 'loungey',\n",
       " 'mist',\n",
       " 'companies',\n",
       " 'double-knot',\n",
       " 'checkboard-like',\n",
       " 'the-place',\n",
       " 'kil',\n",
       " 'tmi',\n",
       " 'descided',\n",
       " 'slip-it',\n",
       " 'practicing',\n",
       " 'no-curve',\n",
       " 'vulnerability',\n",
       " 'a-symmetrical',\n",
       " 'ldecided',\n",
       " 'cirque',\n",
       " 'amt',\n",
       " \"i'lll\",\n",
       " 'ev',\n",
       " \"it'a\",\n",
       " 'scrumptious',\n",
       " 'wear-very',\n",
       " 'multi-color',\n",
       " 'monroe',\n",
       " 'rinestones',\n",
       " 'bottomed',\n",
       " 'anyday',\n",
       " 'cheered',\n",
       " 'madonna',\n",
       " \"lapel's\",\n",
       " 'soup',\n",
       " 'stick-on',\n",
       " 'keer',\n",
       " 'humiliated',\n",
       " 'back-side',\n",
       " 'adirondacks',\n",
       " 'thongs',\n",
       " 'beaufully',\n",
       " 'collects',\n",
       " 'demarcated',\n",
       " 'chamois',\n",
       " 'pessimist',\n",
       " 'loely',\n",
       " 'much-and',\n",
       " 'bursting',\n",
       " 'bulking',\n",
       " 'steamy',\n",
       " 'ping',\n",
       " 'shirt-like',\n",
       " 'sassing',\n",
       " 'heavens',\n",
       " 'salesclerk',\n",
       " 'whisking',\n",
       " 'small-boned',\n",
       " 'cub',\n",
       " 'ls',\n",
       " 'execute',\n",
       " 'difficu',\n",
       " 'midi-top',\n",
       " 'committed',\n",
       " 'arose',\n",
       " 'attent',\n",
       " 'latigo',\n",
       " 'breathability',\n",
       " \"grandpa's\",\n",
       " 'plaform',\n",
       " 'teaser',\n",
       " 'poorly-made',\n",
       " 'hip-less',\n",
       " 'superfast',\n",
       " 'rib-like',\n",
       " 'prado',\n",
       " 'snowstorm',\n",
       " 'bridging',\n",
       " 'model-thin',\n",
       " 'seperate',\n",
       " 'athletic-wear',\n",
       " 'peekaboos',\n",
       " 'sectioning',\n",
       " 'brests',\n",
       " 'peplu',\n",
       " 'unavoidable',\n",
       " 'clarify',\n",
       " 'thigh-length',\n",
       " 'depend',\n",
       " 'middi',\n",
       " 'oceanside',\n",
       " 'flail',\n",
       " 'completes',\n",
       " 'small-of',\n",
       " 'wih',\n",
       " 'abercrombie',\n",
       " 'goings-about',\n",
       " 'groups',\n",
       " 'daughter-in',\n",
       " 'rasberry',\n",
       " 'materi',\n",
       " 'hangy',\n",
       " 'gamut',\n",
       " 'besid',\n",
       " 'fifty',\n",
       " 'minneapol',\n",
       " 'neighbor',\n",
       " 'controls',\n",
       " 'sho',\n",
       " 'cheapish',\n",
       " 'fleetwoods',\n",
       " 'storage',\n",
       " 'bsck',\n",
       " 'proble',\n",
       " 'wax',\n",
       " 'wildberry',\n",
       " 'steadly',\n",
       " 'coat-jackets',\n",
       " 'hassel',\n",
       " 'comfy-cozy',\n",
       " 'roaring',\n",
       " 'temperate',\n",
       " 'normal-length',\n",
       " 'stocked',\n",
       " 'classically',\n",
       " 'bcasue',\n",
       " 'semi-flowy',\n",
       " 'rat',\n",
       " 'prohibitively',\n",
       " 'michelin',\n",
       " 'wrestler',\n",
       " 'drapy',\n",
       " 'realistically',\n",
       " 'backless',\n",
       " 'time-hand',\n",
       " 'assessed',\n",
       " 'bronze-y',\n",
       " 'hood-the',\n",
       " 'nutmeg',\n",
       " 'heartbreaker',\n",
       " 'fetching',\n",
       " 'diving',\n",
       " 'exampl',\n",
       " 'perspiring',\n",
       " 'necklne',\n",
       " 'wor',\n",
       " 'semi-high',\n",
       " 'unadjustable',\n",
       " 'distinctly',\n",
       " 'curled',\n",
       " 'regal',\n",
       " 'dynamic',\n",
       " 'coverall',\n",
       " 'cringed',\n",
       " 'teeny-bop',\n",
       " 'marala',\n",
       " 'one-star',\n",
       " 'insirpring',\n",
       " 'a-lign',\n",
       " 'specs',\n",
       " 'flesh-baring',\n",
       " 'accomadate',\n",
       " 'stuff-and',\n",
       " 'hosiery',\n",
       " 'you-wow',\n",
       " 'brutal',\n",
       " 'louis',\n",
       " 'end-of',\n",
       " 'miss-stitch',\n",
       " 'zeros',\n",
       " 'agent',\n",
       " 'horticulture',\n",
       " 'maxed',\n",
       " \"fisher's\",\n",
       " 'pathetic',\n",
       " 'recreation',\n",
       " 'refrain',\n",
       " 'gorgeus',\n",
       " 'framing',\n",
       " 'flattereing',\n",
       " 'estimating',\n",
       " \"there'd\",\n",
       " 'joes',\n",
       " 'lure',\n",
       " 'globally',\n",
       " 'corn',\n",
       " 'chock',\n",
       " 'bruises',\n",
       " 'twisting',\n",
       " 'nail',\n",
       " 'perks',\n",
       " 'enhancement',\n",
       " 'anything-tights',\n",
       " \"person's\",\n",
       " 'brocades',\n",
       " 'squirrel',\n",
       " 'vaired',\n",
       " 'sided',\n",
       " 'lovethis',\n",
       " 'neccissarily',\n",
       " 'glory',\n",
       " 'right-hand',\n",
       " 'somethig',\n",
       " 'mashing',\n",
       " 'retailerpolgies',\n",
       " 'justifying',\n",
       " 'spruces',\n",
       " 'phd',\n",
       " 'chandelier',\n",
       " 'patched',\n",
       " 'acceptably',\n",
       " 'tights-like',\n",
       " 'lokka',\n",
       " 'assurance',\n",
       " 'dressey',\n",
       " 'closed-end',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the less frequent words\n",
    "lessFreqWords = set(term_freq.hapaxes())\n",
    "lessFreqWords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lessFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLessFreqWords(review):\n",
    "    return [w for w in review if w not in lessFreqWords]\n",
    "\n",
    "tk_reviews = [removeLessFreqWords(review) for review in tk_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7549\n",
      "Total number of tokens:  445958\n",
      "Lexical diversity:  0.016927603047820646\n",
      "Total number of reviews: 19662\n",
      "Average review length: 22.681212491099583\n",
      "Maximun review length: 51\n",
      "Minimun review length: 1\n",
      "Standard deviation of review length: 9.958115518909024\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.7. Remove most frequent words**: Remove the top 20 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2 = list(chain.from_iterable([set(review) for review in tk_reviews]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 6416),\n",
       " ('size', 5888),\n",
       " ('fit', 5537),\n",
       " ('dress', 5346),\n",
       " ('wear', 4900),\n",
       " ('top', 4670),\n",
       " ('great', 4497),\n",
       " ('fabric', 3712),\n",
       " ('color', 3604),\n",
       " ('small', 3265),\n",
       " ('ordered', 3099),\n",
       " ('perfect', 2973),\n",
       " ('flattering', 2939),\n",
       " ('soft', 2805),\n",
       " ('comfortable', 2597),\n",
       " ('back', 2538),\n",
       " ('cute', 2398),\n",
       " ('fits', 2394),\n",
       " ('nice', 2393),\n",
       " ('bought', 2376)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_fd = FreqDist(words_2)  # compute document frequency for each unique word/type\n",
    "mostCommonFredWords = doc_fd.most_common(20) # top 20 most frequent words\n",
    "mostCommonFredWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'size',\n",
       " 'fit',\n",
       " 'dress',\n",
       " 'wear',\n",
       " 'top',\n",
       " 'great',\n",
       " 'fabric',\n",
       " 'color',\n",
       " 'small',\n",
       " 'ordered',\n",
       " 'perfect',\n",
       " 'flattering',\n",
       " 'soft',\n",
       " 'comfortable',\n",
       " 'back',\n",
       " 'cute',\n",
       " 'fits',\n",
       " 'nice',\n",
       " 'bought']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the words from the most common words\n",
    "most_common_words = [word for word, freq in mostCommonFredWords]\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMostFreqWords(review):\n",
    "    return [w for w in review if w not in most_common_words]\n",
    "\n",
    "tk_reviews = [removeMostFreqWords(review) for review in tk_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7529\n",
      "Total number of tokens:  355505\n",
      "Lexical diversity:  0.021178323792914306\n",
      "Total number of reviews: 19662\n",
      "Average review length: 18.080815786796865\n",
      "Maximun review length: 47\n",
      "Minimun review length: 0\n",
      "Standard deviation of review length: 8.833524535391433\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.8. Save processed data**: Store the processed reviews in a CSV file named \"processed.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review:\n",
      " I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments! \n",
      "\n",
      "Tokenized review:\n",
      " ['jumpsuit', 'fun', 'flirty', 'fabulous', 'time', 'compliments']\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw review:\\n\",reviews[test_index],'\\n')\n",
    "print(\"Tokenized review:\\n\",tk_reviews[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpsuit fun flirty fabulous time compliments'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_reviews = [' '.join(review) for review in tk_reviews]\n",
    "joined_reviews[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>high hopes wanted work initially petite usual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>jumpsuit fun flirty fabulous time compliments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>shirt due adjustable front tie length leggings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>tracy reese dresses petite feet tall brand pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>basket hte person store pick teh pale hte gorg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0         General         Dresses    Dresses   \n",
       "1                        0  General Petite         Bottoms      Pants   \n",
       "2                        6         General            Tops    Blouses   \n",
       "3                        4         General         Dresses    Dresses   \n",
       "4                        1  General Petite            Tops      Knits   \n",
       "\n",
       "                               Processed Review Text  \n",
       "0  high hopes wanted work initially petite usual ...  \n",
       "1      jumpsuit fun flirty fabulous time compliments  \n",
       "2  shirt due adjustable front tie length leggings...  \n",
       "3  tracy reese dresses petite feet tall brand pre...  \n",
       "4  basket hte person store pick teh pale hte gorg...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We save processed reviews to a file with the new column name\n",
    "df['Processed Review Text'] = joined_reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in process reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the count of null values in both DataFrames\n",
    "print(f\"Null values in process reviews: {df['Processed Review Text'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty strings in process reviews: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19662, 11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for empty strings too\n",
    "print(f\"Empty strings in process reviews: {(df['Processed Review Text'] == '').sum()}\")\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null or empty strings in the 'Processed Review Text' column because they are not useful for our analysis\n",
    "df = df[(df['Processed Review Text'].notna()) & (df['Processed Review Text'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty strings in process reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# Check it again\n",
    "print(f\"Empty strings in process reviews: {(df['Processed Review Text'] == '').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 11)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to a new CSV file\n",
    "df.to_csv('processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the requested information as per specification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7529\n",
      "Total number of tokens:  355505\n",
      "Lexical diversity:  0.021178323792914306\n",
      "Total number of reviews: 19662\n",
      "Average review length: 18.080815786796865\n",
      "Maximun review length: 47\n",
      "Minimun review length: 0\n",
      "Standard deviation of review length: 8.833524535391433\n"
     ]
    }
   ],
   "source": [
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7529"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating the vocabulary\n",
    "words_3 = list(chain.from_iterable(tk_reviews)) # we put all the tokens in the corpus in a single list\n",
    "vocab = sorted(list(set(words_3))) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "vocab[:10] # print the first 10 words in the vocabulary\n",
    "len(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary to vocab.txt\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for i, word in enumerate(vocab):\n",
    "        f.write(f\"{word}:{i}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Activities and labs files for this course. \n",
    "- Github Copilot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
